{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Для решения задачи рекомендации на основе MovieLens я выбрал библиотеку Pytorch и подход NCF. Мы будем тестироваться на рекомендации(подборке) 10 фильмов для каждого пользователя.Данные для обучения я предварительно загрузил на гугл диск для удобного доступа из colab. Далее, в классе UserItemRatingDataset я сделал представление наших данных в виде torch.tensor.\n",
        "\n",
        "Я решил перевести Excplicit feedback в Implicit(т.е. оценки пользователями фильмов в бинарную метрику поставил оценку/не поставил), обычно Implicit feedbackа гораздо больше, поэтому было бы разумно сразу обучать модель так. \n",
        "В классе NCFData я создаю данные для обучения/тестирования. Важно отметить, что нам нужно вручную создать для пользователей отрицательные примеры(сейчас мы знаем только про положительные, т.е. оценки от пользователей). Для этого случайно будет добавлять к пользователем фильмы, которые он не оценил.\n",
        "Для тестирования будем использовать подход Leave One Out, то есть тестировать на основе последней оценки фильма от пользователя.\n",
        "\n",
        "К сожалению, при полной загрузке датасета MovieLens 25m вылетает оперативная память, так что я ограничусь примерно 500к оценок(если необходимо, при больших вычислительных мощностях можно засунуть и полный датасет).\n",
        "\n",
        "Для оценки качества модели я выбрал метрики Hit(попадание релевантного элемента в рекомендуемые, без учета их ранжирования) и nDCG(normalized Discounted Cumulative Gain), где уже учитывается ранжирование(т.е. более релевантные элементы должны быть выше в списке рекомендаций). \n",
        "\n",
        "Далее я реализовал модель GMF(матричное разложение матрицы пользователей и фильмов). С помощью nn.Embeddings мы создаем два эмбеддинга пользователей и фильмов, перемножаем их поэлементно и пропускаем через полносвязный слой с активацией сигмоида.\n",
        "\n",
        "В конце, обучаем нашу нейронную сеть с помощью train_pipeline и смотрим на средние значения метрик Hit и nDCG на каждой эпохе.\n",
        "Уже после нескольких эпох мы достигаем неплохо качества Hit и nDCG(порядка 0.65-0.7 и 0.4 где-то на 7 эпохе). Это достаточно хорошо, так как получается для 70% пользователей мы смогли порекомендовать такие фильмы, что они посмотрели и оценили хотя-бы один из них."
      ],
      "metadata": {
        "id": "Ynj4zpxrHZya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "collapsed": true,
        "id": "5Hq0fDqXN8Lp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL9jn8aRwpif",
        "outputId": "e193ac0e-dd1c-4e80-8455-3ccf1c5e8336"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "TOP_K = 10"
      ],
      "metadata": {
        "id": "uEGUFBbtN8Lu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "outputs": [],
      "source": [
        "class UserItemRatingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Делаем тензоры из наших данных\n",
        "    \"\"\"\n",
        "    def __init__(self, user_list, item_list, rating_list):\n",
        "        super(UserItemRatingDataset, self).__init__()\n",
        "        self.user_tensor = torch.tensor(user_list, dtype=torch.long)\n",
        "        self.item_tensor = torch.tensor(item_list, dtype=torch.long)\n",
        "        self.target_tensor = torch.tensor(rating_list, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_tensor)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.user_tensor[idx], self.item_tensor[idx], self.target_tensor[idx]"
      ],
      "metadata": {
        "id": "lDyh9HZqN8Lu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-129-17bf4d94cfbb>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "class NCFData(object):\n",
        "    \"\"\"\n",
        "    Собираем данные для обучения модели\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratings, num_negatives, num_negatives_test, batch_size):\n",
        "        self.ratings = ratings\n",
        "        self.num_negatives = num_negatives\n",
        "        self.num_negatives_test = num_negatives_test\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.preprocess_ratings = self._reindex(self.ratings)\n",
        "        self.user_pool = set(self.ratings['user_id'].unique())\n",
        "        self.item_pool = set(self.ratings['item_id'].unique())\n",
        "\n",
        "        self.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n",
        "        self.negatives = self._negative_sampling(self.preprocess_ratings)\n",
        "\n",
        "    def _reindex(self, ratings):\n",
        "        \"\"\"\n",
        "        Перенумеровываем данные, так как изначально у нас идут произвольные индексы\n",
        "        \"\"\"\n",
        "        user_list = list(ratings['user_id'].drop_duplicates())\n",
        "        self.user2id = {w: i for i, w in enumerate(user_list)}\n",
        "\n",
        "        item_list = list(ratings['item_id'].drop_duplicates())\n",
        "        self.item2id = {w: i for i, w in enumerate(item_list)}\n",
        "\n",
        "        ratings['user_id'] = ratings['user_id'].apply(lambda x: self.user2id[x])\n",
        "        ratings['item_id'] = ratings['item_id'].apply(lambda x: self.item2id[x])\n",
        "        ratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
        "        return ratings\n",
        "\n",
        "    def _leave_one_out(self, ratings):\n",
        "        \"\"\"\n",
        "        Хотим тестировать на последнем фильме, который пользователь оценил\n",
        "        \"\"\"\n",
        "        ratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
        "        test = ratings.loc[ratings['rank_latest'] == 1]\n",
        "        train = ratings.loc[ratings['rank_latest'] > 1]\n",
        "        test = test[test['user_id'].isin(train['user_id'].unique())]\n",
        "        print(train['user_id'].nunique(), test['user_id'].nunique())\n",
        "        assert train['user_id'].nunique() == test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
        "        return train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
        "\n",
        "    def _negative_sampling(self, ratings):\n",
        "        \"\"\"\n",
        "        Для всех пользователей создаем какие-то негативные примеры(т.е. фильмы, которые он ещё не оценивал)\n",
        "        \"\"\"\n",
        "        interact_status = (ratings.groupby('user_id')['item_id'].apply(set).reset_index().rename(\n",
        "            columns={'item_id': 'interacted_items'}))\n",
        "        interact_status['negative_items'] = (interact_status['interacted_items'].apply(lambda x: self.item_pool - x))\n",
        "        interact_status['negative_samples'] = (\n",
        "            interact_status['negative_items'].apply(lambda x: random.choices(tuple(x), k=self.num_negatives_test)))\n",
        "        return interact_status[['user_id', 'negative_items', 'negative_samples']]\n",
        "\n",
        "    def get_train_instance(self):\n",
        "        \"\"\"\n",
        "        Собираем это все вместе и конструируем данные для обучения\n",
        "        \"\"\"\n",
        "        users, items, ratings = [], [], []\n",
        "        train_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n",
        "        train_ratings['negatives'] = train_ratings['negative_items'].apply(\n",
        "            lambda x: random.choices(tuple(x), k=self.num_negatives))\n",
        "        for row in train_ratings.itertuples():\n",
        "            users.append(int(row.user_id))\n",
        "            items.append(int(row.item_id))\n",
        "            ratings.append(float(row.rating))\n",
        "            for i in range(self.num_negatives):\n",
        "                users.append(int(row.user_id))\n",
        "                items.append(int(row.negatives[i]))\n",
        "                ratings.append(float(0))  # negative samples get 0 rating\n",
        "\n",
        "        dataset = UserItemRatingDataset(user_list=users, item_list=items, rating_list=ratings)\n",
        "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    def get_test_instance(self):\n",
        "        \"\"\"\n",
        "        Собираем данные примерно так же для тестирования\n",
        "        \"\"\"\n",
        "        users, items, ratings = [], [], []\n",
        "        test_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
        "        for row in test_ratings.itertuples():\n",
        "            users.append(int(row.user_id))\n",
        "            items.append(int(row.item_id))\n",
        "            ratings.append(float(row.rating))\n",
        "            for i in getattr(row, 'negative_samples'):\n",
        "                users.append(int(row.user_id))\n",
        "                items.append(int(i))\n",
        "                ratings.append(float(0))\n",
        "\n",
        "        dataset = UserItemRatingDataset(user_list=users, item_list=items, rating_list=ratings)\n",
        "        return DataLoader(dataset, batch_size=self.num_negatives_test + 1, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "AMheJW5TN8Lv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "c87ed28d-c173-402b-e004-9ad15975a47e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml = pd.read_csv('drive/MyDrive/Colab Notebooks/ml-latest/ratings.csv').rename(columns={'userId': 'user_id', 'movieId': 'item_id'})"
      ],
      "metadata": {
        "id": "JLhpej601Byx"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Я изменил тип данных в каждой из колонок чтобы сэкономить оперативную память(но это не помогло)\n",
        "#После этого просто обрезал датасет\n",
        "ml['rating'] = (ml['rating'] * 2).astype('int8')\n",
        "ml['user_id'] = ml['user_id'].astype('int32')\n",
        "ml['item_id'] = ml['item_id'].astype('int32')\n",
        "ml = ml.iloc[:27753444//50]"
      ],
      "metadata": {
        "id": "ITUZCGFpybyV"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "outputs": [],
      "source": [
        "#получим количество фильмов и пользователей\n",
        "num_users = ml['user_id'].nunique() + 1\n",
        "num_items = ml['item_id'].nunique() + 1"
      ],
      "metadata": {
        "id": "47WpUazZN8Lx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-118-2b387ffdc322>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ratings['user_id'] = ratings['user_id'].apply(lambda x: self.user2id[x])\n",
            "<ipython-input-118-2b387ffdc322>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ratings['item_id'] = ratings['item_id'].apply(lambda x: self.item2id[x])\n",
            "<ipython-input-118-2b387ffdc322>:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
            "<ipython-input-118-2b387ffdc322>:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  ratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5498 5498\n"
          ]
        }
      ],
      "source": [
        "data = NCFData(ml, num_negatives=4, num_negatives_test=100, batch_size=1024)"
      ],
      "metadata": {
        "id": "zT7VQIO2N8L0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c9d761-38ed-48f9-d0b6-2b5276f67d5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-130-e94e13bf7b8e>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "def hit(ng_item, pred_items):\n",
        "    \"\"\"\n",
        "    Метрика попадание(без учета ранжирования)\n",
        "    \"\"\"\n",
        "    if ng_item in pred_items:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "\n",
        "def ndcg(ng_item, pred_items):\n",
        "    \"\"\"\n",
        "    Метрика попадния с учетом ранжирования\n",
        "    \"\"\"\n",
        "    if ng_item in pred_items:\n",
        "        index = pred_items.index(ng_item)\n",
        "        return np.reciprocal(np.log2(index + 2))\n",
        "    return 0\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def metrics(model, test_loader, top_k, device):\n",
        "    \"\"\"\n",
        "    Соберем все метрики для каждого пользователя и усредним\n",
        "    \"\"\"\n",
        "    _hr, _ndcg = [], []\n",
        "\n",
        "    for user, item, label in test_loader:\n",
        "        user = user.to(device)\n",
        "        item = item.to(device)\n",
        "\n",
        "        predictions = model(user, item)\n",
        "        predictions = predictions.view(-1)\n",
        "        _, indices = torch.topk(predictions, top_k)\n",
        "        recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
        "\n",
        "        ng_item = item[0].item()  # leave one-out evaluation has only one item per user\n",
        "        _hr.append(hit(ng_item, recommends))\n",
        "        _ndcg.append(ndcg(ng_item, recommends))\n",
        "\n",
        "    return np.mean(_hr), np.mean(_ndcg)"
      ],
      "metadata": {
        "id": "kiEReIMKN8L1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "f2ae03fc-b7e5-4d38-ce23-ab37e34b1a1c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "outputs": [],
      "source": [
        "class GMF(nn.Module):\n",
        "    \"\"\"\n",
        "    Строим архитектуру GMF как описывали в начале\n",
        "    \"\"\"\n",
        "    def __init__(self, num_users, num_items, embedding_dim):\n",
        "        super(GMF, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.embedding_user = nn.Embedding(num_embeddings=num_users, embedding_dim=embedding_dim)\n",
        "        self.embedding_item = nn.Embedding(num_embeddings=num_items, embedding_dim=embedding_dim)\n",
        "        self.affine_output = nn.Linear(in_features=embedding_dim, out_features=1)\n",
        "        self.logistic = nn.Sigmoid()\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        element_product = torch.mul(user_embedding, item_embedding)\n",
        "        logits = self.affine_output(element_product)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        \"\"\"\n",
        "        Для улучшения обучения и получения быстрее хороших значений метрик инициализируем веса с помощью Xavier Uniform\n",
        "        \"\"\"\n",
        "        nn.init.xavier_uniform_(self.embedding_user.weight)\n",
        "        nn.init.xavier_uniform_(self.embedding_item.weight)"
      ],
      "metadata": {
        "id": "77xClpNCN8L1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "outputs": [],
      "source": [
        "def train_pipeline(model, optimizer, criterion, data, num_epochs):\n",
        "    \"\"\"\n",
        "    Обучаем модель, с накоплением истории изменения метрик и удобным выводом с помощью tqdm\n",
        "    \"\"\"\n",
        "    loss_history = []\n",
        "    metrics_history = {'HR@10': [], 'NDCG@10': []}\n",
        "    test_loader = data.get_test_instance()\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train() \n",
        "        train_loader = data.get_train_instance()\n",
        "\n",
        "        for user, item, label in tqdm.tqdm(train_loader, desc=f'[Epoch #{epoch}]',total=len(train_loader)):\n",
        "            user = user.to(DEVICE)\n",
        "            item = item.to(DEVICE)\n",
        "            label = label.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            prediction = model(user, item)\n",
        "\n",
        "            loss = criterion(prediction.view(-1), label.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_history.append(loss.item())\n",
        "\n",
        "        # Накапливаем метрики\n",
        "        model.eval()\n",
        "        hr_i, ndcg_i = metrics(model, test_loader, TOP_K, DEVICE)\n",
        "        metrics_history['HR@10'].append(hr_i)\n",
        "        metrics_history['NDCG@10'].append(ndcg_i)\n",
        "\n",
        "        print(f\"[Epoch #{epoch}] HR: {hr_i:.3f}\\ndcg: {ndcg_i:.3f}\")\n",
        "\n",
        "    return loss_history, metrics_history"
      ],
      "metadata": {
        "id": "-KMqCHSjN8L2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "outputs": [],
      "source": [
        "#Выбираем как критерий бинарную кросс энтропию что типично для задач бинарной классификации\n",
        "# И стандартный оптимизатор Adam\n",
        "model = GMF(num_users=num_users, num_items=num_items, embedding_dim=32)\n",
        "model = model.to(DEVICE)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "1m3AN-KSN8L2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch #1]: 100%|██████████| 2683/2683 [03:37<00:00, 12.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch #1] HR: 0.775\n",
            "dcg: 0.488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Epoch #2]:  49%|████▉     | 1324/2683 [01:53<01:56, 11.69it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-7e8d4d807fbf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-126-0556b4e1213f>\u001b[0m in \u001b[0;36mtrain_pipeline\u001b[0;34m(model, optimizer, criterion, data, num_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Train Whole Epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#for user, item, label in train_loader:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'[Epoch #{epoch}]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1186\u001b[0m                     \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcur_t\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_print_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmininterval\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcur_t\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_start_t\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_print_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m                         \u001b[0mlast_print_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                         \u001b[0mlast_print_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_print_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ema_dt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_miniters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m                     \u001b[0;31m# If no `miniters` was specified, adjust automatically to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/utils.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_history, metrics_history = train_pipeline(model, optimizer, criterion, data, num_epochs=10)"
      ],
      "metadata": {
        "id": "ZJumEWlMN8L3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "dd041344-884c-4a4e-ee3b-699fe23c4b7a"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}